# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0

from aiconfigurator.sdk.models import get_model
from aiconfigurator.sdk.perf_database import get_database
import sys
import argparse
from aiconfigurator.sdk import common
from aiconfigurator.sdk import config
from aiconfigurator.sdk.backends.factory import get_backend
from aiconfigurator.sdk.inference_session import InferenceSession
import os

def parse(args):
    parser = argparse.ArgumentParser()
    parser.add_argument("--batch_size",
                        type=int,
                        help="batch_size")
    parser.add_argument("--isl",
                        type=int,
                        help="input sequence length, max 256k")
    parser.add_argument("--osl",
                        type=int,
                        help="output sequence length")
    parser.add_argument("--stride",
                        type=int,
                        default=32,
                        help="stride for fast generation, 32 by default")   
    parser.add_argument("--mode",
                        type=str,
                        default='static',
                        choices=['static', 'static_ctx', 'static_gen', 'agg', 'disagg'],
                        help='inference mode')
    parser.add_argument("--backend",
                        type=str,
                        default='trtllm',
                        choices=list(common.BackendName.__members__.keys()),
                        help='backend')
    parser.add_argument("--version",
                        type=str,
                        default='0.20.0',
                        help='backend version')
    parser.add_argument("--system",
                        type=str,
                        default='h200_sxm',
                        choices=['h100_sxm', 'h200_sxm'],
                        help='GPU+system Type')
    parser.add_argument("--model",
                        type=str,
                        choices=list(common.SupportedModels.keys()),
                        help='model name, with size')
    parser.add_argument("--overwrite_num_layers",
                        type=int,
                        default=0,
                        help='if larger than 0, overwrite model layers to the value assigned')
    parser.add_argument("--attention_dp_size",
                        type=int,
                        default=1,
                        help="attention data parallel size  moe models")
    parser.add_argument("--tp_size",
                        type=int,
                        default=1,
                        choices=[1,2,4,8,16,32],
                        help="tensor parallel size, support 1,2,4,8,16,32")
    parser.add_argument("--pp_size",
                        type=int,
                        default=1,
                        choices=[1,2,4,8],
                        help="pipeline parallel size, support 1,2,4,8")
    parser.add_argument("--gemm_quant_mode",
                        type=str,
                        default='float16',
                        choices=list(common.GEMMQuantMode.__members__.keys()),
                        help='gemm quantization type')
    parser.add_argument("--moe_quant_mode",
                        type=str,
                        default='float16',
                        choices=list(common.MoEQuantMode.__members__.keys()),
                        help='moe quantization type')
    parser.add_argument("--kvcache_quant_mode",
                        type=str,
                        default='float16',
                        choices=list(common.KVCacheQuantMode.__members__.keys()),
                        help='kvcache quantization type')
    parser.add_argument("--fmha_quant_mode",
                        type=str,
                        default='float16',
                        choices=list(common.FMHAQuantMode.__members__.keys()),
                        help='kvcache quantization type')    
    parser.add_argument("--moe_tp_size",
                        type=int,
                        default=1,
                        help="tp size for moe")
    parser.add_argument("--moe_ep_size",
                        type=int,
                        default=1,
                        help="ep size for moe")
    parser.add_argument("--workload_distribution",
                        type=str,
                        default='uniform',
                        choices=['uniform','balanced','mostUnbalanced'],
                        help="workload for moe models")
    parser.add_argument("--ttft",
                        type=float,
                        default=300,
                        help="SLA requirement: ttft limit (ms)")
    parser.add_argument("--tpot",
                        type=float,
                        default=50,
                        help="SLA requirement: tpot limit (ms)")
    parser.add_argument("--ctx_stride",
                        type=int,
                        default=64,
                        help="Agg: ctx_stride, mimicking num_tokens_per_block param in TRT-LLM, defaulted as 64")
    args = parser.parse_args(args=args)

    return args

def main(args):
    database = get_database(system=args.system, backend=args.backend, version=args.version)
    model_config = config.ModelConfig(tp_size=args.tp_size, pp_size=args.pp_size, \
                                      gemm_quant_mode=common.GEMMQuantMode[args.gemm_quant_mode], moe_quant_mode=common.MoEQuantMode[args.moe_quant_mode], \
                                      kvcache_quant_mode=common.KVCacheQuantMode[args.kvcache_quant_mode], fmha_quant_mode=common.FMHAQuantMode[args.fmha_quant_mode], \
                                      moe_tp_size=args.moe_tp_size, moe_ep_size=args.moe_ep_size, workload_distribution=args.workload_distribution, \
                                      attention_dp_size=args.attention_dp_size, \
                                      overwrite_num_layers=args.overwrite_num_layers)
    runtime_config = config.RuntimeConfig(batch_size=args.batch_size, beam_width=1, isl=args.isl, osl=args.osl, ttft=args.ttft, tpot=args.tpot)
    model = get_model(args.model, model_config)
    backend = get_backend(args.backend)
    session = InferenceSession(model, database, backend)

    if args.mode == 'agg':
        summary = session.find_best_agg_result_under_constraints(runtime_config=runtime_config, top_k=0, max_batch_size=512, ctx_stride=args.ctx_stride)
        print(summary.get_summary_df())
        summary.get_summary_df().to_csv('result.csv')
    elif args.mode == 'disagg':
        summary = None # TODO, leave this for future as it requires too many params as input. It should call into DisaggInferenceSession.run_disagg()
    else:
        summary = session.run_static(mode=args.mode, runtime_config=runtime_config, stride=32)
        print(summary.get_summary_df())
        for info in summary.get_static_info():
            print(info)
    
if __name__ == '__main__':
    args = parse(sys.argv[1:])
    main(args)