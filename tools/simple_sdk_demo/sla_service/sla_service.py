# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0

from aiconfigurator.sdk.inference_session import InferenceSession
import pandas as pd
from aiconfigurator.sdk.models import get_model, check_is_moe
from aiconfigurator.sdk.perf_database import get_database
from aiconfigurator.sdk import common
import copy
from aiconfigurator.sdk.config import ModelConfig, RuntimeConfig
from aiconfigurator.sdk.pareto_analysis import enumerate_parallel_config
from fastapi import FastAPI, Response, Body
import orjson
import sys
from typing import Any,Optional
from aiconfigurator.sdk.common import SupportedModels
import uvicorn
import argparse
from aiconfigurator.sdk.backends.factory import get_backend
import logging

logger = logging.getLogger(__name__)

class PrettyJSONResponse(Response):
    media_type = "application/json"
    
    def render(self, content: Any) -> bytes:
        return orjson.dumps(content, option=orjson.OPT_INDENT_2)

app = FastAPI(title="Dynamo AIConfigurator SLA API", description="Dynamo AIConfigurator SLA API", default_response_class=PrettyJSONResponse)

@app.get("/sla/supported_models")
def get_supported_models():
    return Response(content=orjson.dumps({'model list:':list(SupportedModels.keys())}), media_type="application/json")


@app.post("/sla")
def post_sla(
    system: str = Body("h200_sxm", description="hardware name, h200_sxm, h100_sxm"),       
    backend: str = Body("trtllm", description="backend name, trtllm, sglang, vllm"),
    version: str = Body("0.20.0", description="trtllm version, 0.20.0"), 
    model_name: str = Body("QWEN3_32B", description="model name"),
    isl: int = Body(4000, description="input sequence length"),
    osl: int = Body(500, description="output sequence length"),
    ttft: int = Body(300, description="first token latency limit"),
    tpot: int = Body(10, description="inter token latency limit"),
    quant: str = Body("fp8", description="quantization mode: fp8, fp8_block, float16"),
    kvcache_quant: str = Body("fp8", description="kvcache quantization mode, fp8, int8, float16"),
):
    
    logging.basicConfig(level=logging.INFO)
    result_dict = {}
    try:
        model_config = ModelConfig(gemm_quant_mode=common.GEMMQuantMode[quant],
                                        kvcache_quant_mode=common.KVCacheQuantMode[kvcache_quant],
                                        fmha_quant_mode=common.FMHAQuantMode.float16)
        runtime_config = RuntimeConfig(batch_size=1, isl=isl, osl=osl, ttft=ttft, tpot=tpot)

        database = get_database(system, backend, version)
        backend_instance = get_backend(backend)

        # dense model
        is_moe = check_is_moe(model_name)
        agg_parallel_config_list = enumerate_parallel_config(num_gpu_list=[1,2,4,8],
                                                            tp_list=[1,2,4,8],
                                                            pp_list=[1],
                                                            moe_tp_list=[1,2,4,8],
                                                            moe_ep_list=[1,2,4,8],
                                                            dp_list=[1],
                                                            is_moe=is_moe,
                                                            backend=common.BackendName(backend))

        concurrency_list_default = [2,4,8,16,32,48,64,96,128,192,256,384,512,768,1024,2048,3072,4096]
        max_num_tokens = 8192 # default as NIM
        min_cc = max_num_tokens//isl+1
        cc_list = [cc for cc in concurrency_list_default if cc >= min_cc]
        results_df = pd.DataFrame(columns=common.ColumnsAgg)
        for parallel_config in agg_parallel_config_list:
            tp_size, pp_size, dp_size, moe_tp_size, moe_ep_size = parallel_config
            overwritten_model_config = copy.deepcopy(model_config)
            overwritten_model_config.pp_size = pp_size
            overwritten_model_config.tp_size = tp_size
            overwritten_model_config.moe_tp_size = moe_tp_size
            overwritten_model_config.moe_ep_size = moe_ep_size
            overwritten_model_config.attention_dp_size = dp_size
            model = get_model(model_name=model_name, model_config=overwritten_model_config)
            sess = InferenceSession(model, database, backend_instance)

            for cc in cc_list:
                runtime_config.batch_size = cc
                summary = sess.run_agg(runtime_config=runtime_config, ctx_tokens=max_num_tokens)
                result_df = summary.get_summary_df()
                if summary.check_oom():
                    logger.info(f"OOM for cc: {cc}")
                    break # larger cc will cause oom
                if result_df.loc[0,'tpot'] <= tpot and result_df.loc[0,'ttft'] <= ttft:
                    logger.info(f"Found valid config for cc: {cc}")
                    if len(results_df) == 0:
                        results_df = result_df
                    else:
                        results_df = pd.concat([results_df, result_df], axis=0, ignore_index=True)
                else:
                    logger.info(f"Invalid config for cc: {cc} tpot: {result_df.loc[0,'tpot']} ttft: {result_df.loc[0,'ttft']}")
                    break

        results_df = results_df.sort_values(by='tokens/s/gpu', ascending=False).reset_index(drop=True)

        if len(results_df) != 0:
            result_dict = results_df.loc[0].to_dict()
    except Exception as e:
        print(e)
        result_dict = {'error': str(e)}
    
    return result_dict

def parse(args):
    parser = argparse.ArgumentParser()
    parser.add_argument("--server_name",
                        type=str,
                        default='127.0.0.1',
                        help="server name")
    parser.add_argument("--server_port",
                        type=int,
                        default=7860,
                        help="server port")
    args = parser.parse_args(args=args)
    return args
    
if __name__ == '__main__':
    args = parse(sys.argv[1:])
    uvicorn.run(app, host=args.server_name, port=args.server_port)