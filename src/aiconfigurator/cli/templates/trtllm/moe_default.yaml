# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0

# ============================= User config section begin =============================
# select a model from the following list:
# GPT_7B, GPT_13B, GPT_30B, GPT_66B, GPT_175B
# LLAMA2_7B, LLAMA2_13B, LLAMA2_70B, LLAMA3.1_8B, LLAMA3.1_70B, LLAMA3.1_405B
# MOE_Mixtral8x7B, MOE_Mixtral8x22B
# DEEPSEEK_V3
# QWEN2.5_1.5B, QWEN2.5_7B, QWEN2.5_32B, QWEN2.5_72B, QWEN3_32B, QWEN3_235B

# Your scenario and required SLA
isl: 4000 # input sequence length
osl: 1000 # output sequence length
ttft: 500.0  # Target TTFT in ms
tpot: 30.0   # Target TPOT in ms

common_framework_config: &common_framework_config
  backend_name: "trtllm" # trtllm, sglang, vllm
  version: "0.20.0" # based on your local database

agg_system_name: &agg_system_name "h200_sxm" # h200_sxm, h100_sxm, depends on your local database
disagg_prefill_system_name: &disagg_prefill_system_name "h200_sxm" # h200_sxm, h100_sxm, depends on your local database
disagg_decode_system_name: &disagg_decode_system_name "h200_sxm" # h200_sxm, h100_sxm, depends on your local database
# ============================= User config section end =============================

# ----------------------------- no need to modify below --------------------------------


# you can modify system, framework(backend) and version to match your environment
# if you want to modify the quantization config, please refer to common.py in sdk
# ============================= search system config begin =============================
# agg(agg) config
agg_config:
  agg_worker_config:
    system_config:
      system_name: *agg_system_name
      <<: *common_framework_config
    quant_config:
      gemm_quant_mode: "fp8_block" # fp8, fp8_block, float16
      moe_quant_mode: "fp8_block" # fp8, fp8_block, w4afp8, float16
      kvcache_quant_mode: "fp8" # fp8, int8, float16
      fmha_quant_mode: "fp8" # fp8, float16
      comm_quant_mode: "half" # half
    parallel_config:
      num_gpu_per_worker: [1, 2, 4, 8]
      tp_list: [1, 2, 4, 8]
      pp_list: [1]
      dp_list: [1, 2, 4, 8]
      moe_tp_list: [1]
      moe_ep_list: [1, 2, 4, 8]

# disagg config
disagg_config:
  # the whole replica config, a replica is the minimum unit of disagg deployment. It contains xPyD workers.
  # x is the number of prefill workers, y is the number of decode workers
  # then we scale replicas to meet your total gpus requirement.
  replica_config:
    num_gpu_per_replica: [2, 4, 8, 16, 24, 32, 40, 48, 56, 64] # It means the searched replica will have total gpus in this list, this list will be capped by max_gpu_per_replica
    max_gpu_per_replica: 64 # max gpus per replica, if specified as 0, it means no limit. Too many gpus per replica will make the prefill/decoder worker pair complicated. no need to be too large.
    max_prefill_worker: 32 # It means in every replica, you will have up to 32 prefill workers, x_max = 32
    max_decode_worker: 32 # It means in every replica, you will have up to 32 decode workers, y_max = 32
  # each prefill worker config
  prefill_worker_config:
    system_config:
      system_name: *disagg_prefill_system_name
      <<: *common_framework_config
    quant_config:
      gemm_quant_mode: "fp8_block" # fp8, fp8_block, float16
      moe_quant_mode: "fp8_block" # fp8, fp8_block, w4afp8, float16
      kvcache_quant_mode: "fp8" # fp8, int8, float16
      fmha_quant_mode: "fp8" # fp8, float16
      comm_quant_mode: "half" # half
    parallel_config:
      num_gpu_per_worker: [1, 2, 4, 8]
      tp_list: [1, 2, 4, 8]
      pp_list: [1]
      dp_list: [1] # we didn't enable attn dp here. You can enable it if you want.
      moe_tp_list: [1]
      moe_ep_list: [1, 2, 4, 8]
  # each decode worker config
  decode_worker_config:
    system_config:
      system_name: *disagg_decode_system_name
      <<: *common_framework_config
    quant_config:
      gemm_quant_mode: "fp8_block" # fp8, fp8_block, float16
      moe_quant_mode: "fp8_block" # fp8, fp8_block, w4afp8, float16
      kvcache_quant_mode: "fp8" # fp8, int8, float16
      fmha_quant_mode: "fp8" # fp8, float16
      comm_quant_mode: "half" # half
    parallel_config:
      num_gpu_per_worker: [1, 2, 4, 8]
      tp_list: [1, 2, 4, 8]
      pp_list: [1]
      dp_list: [1, 2, 4, 8]
      moe_tp_list: [1]
      moe_ep_list: [1, 2, 4, 8]

# advanced tuning config
prefill_correction_scale: 1.0 # If you find the predicted prefill perf is too optimistic, you can set a scale factor to make it more realistic, throughput_corrected = throughput_predicted * prefill_correction_scale
decode_correction_scale: 1.0 # If you find the predicted decode perf is too optimistic, you can set a scale factor to make it more realistic, throughput_corrected = throughput_predicted * decode_correction_scale
prefill_max_batch_size: 1
decode_max_batch_size: 512