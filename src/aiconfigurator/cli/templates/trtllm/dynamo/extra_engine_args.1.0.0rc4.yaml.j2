backend: pytorch

{% if is_moe is defined %}  # is_moe from sdk
moe_expert_parallel_size: {{ moe_ep }}
moe_tensor_parallel_size: {{ moe_tp }}

moe_config:
    backend: {{ dynamo_config.moe_backend | default('CUTLASS') }} # dynamo_config.moe_backend MOE backend to use (CUTLASS, CUTEDSL, WIDEEP, TRTLLM, VANILLA)
    {% if dynamo_config.moe_load_balancer is defined %}  # Configuration for MoE load balancing
    load_balancer: {{ dynamo_config.moe_load_balancer }} # moe load balancer
    {% endif %}
{% endif %}

tensor_parallel_size: {{ tp }}
pipeline_parallel_size: {{ pp }}
enable_attention_dp: {{ enable_attention_dp | default(false) }}
enable_chunked_prefill: {{ dynamo_config.enable_chunked_prefill | default(false) }}

max_batch_size: {{ bs }}
max_num_tokens: {{ max_num_tokens }}
max_seq_len: {{ max_seq_len }}

kv_cache_config:
  enable_block_reuse: {{ dynamo_config.enable_block_reuse | default(true) }} # dynamo_config.enable_block_reuse Controls if KV cache can be reused for different requests
  free_gpu_memory_fraction: {{ dynamo_config.free_gpu_memory_fraction | default(0.80) }}  # dynamo_config.free_gpu_memory_fraction The maximum number of tokens that should be stored in the KV cache.
  dtype: {{ kv_cache_dtype | default('auto') }}

cache_transceiver_config:
  backend: {{ dynamo_config.cache_transceiver_backend | default('default') }} # dynamo_config.cache_transceiver_backend The communication backend type to use for the cache transceiver (default, ucx, nixl, mpi)

cuda_graph_config:
  enable_padding: {{ dynamo_config.cuda_graph_padding_enabled | default(true) }} # dynamo_config.cuda_graph_padding_enabled Pad CUDA graph
  batch_sizes: [{% for s in dynamo_config.cuda_graph_batch_sizes
                               | default([1,2,4,8,16,32,64,128,256]) -%}
                    {{- s }}{{ ',' if not loop.last else '' }} {%- endfor %}]

disable_overlap_scheduler: {{ dynamo_config.disable_overlap_scheduler | default(true) }}
print_iter_log: {{ dynamo_config.print_iter_log | default(false) }}  # dynamo_config.print_iter_log Print iteration logs

{% if is_speculative is defined %}  # speculative decoding
speculative_config:
  decoding_type: {{ decoding_type }}
  num_nextn_predict_layers: {{ num_nextn_predict_layers }}
{% endif %}