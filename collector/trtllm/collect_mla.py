# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0

import os
import torch

import tensorrt_llm
from tensorrt_llm.mapping import Mapping
from tensorrt_llm._torch.attention_backend.utils import create_attention
from tensorrt_llm._torch.attention_backend.interface import PositionalEmbeddingParams, RopeParams, AttentionRuntimeFeatures
from tensorrt_llm.functional import PositionEmbeddingType, RotaryScalingType
from tensorrt_llm.models.modeling_utils import QuantConfig
from tensorrt_llm._torch.attention_backend import TrtllmAttentionMetadata, AttentionInputType
from tensorrt_llm._torch.pyexecutor.resource_manager import KVCacheManager
from tensorrt_llm._torch.metadata import KVCacheParams
from tensorrt_llm.llmapi import KvCacheConfig
from helper import log_perf

def get_context_mla_test_cases():
    dtype_list = [tensorrt_llm.bindings.DataType.FP8,tensorrt_llm.bindings.DataType.BF16]
    test_cases = []
    n_list = [64, 128]
    b_list = [1,2,4,8,16,32,64,128,256]
    s_list = [16,32,64,128,256,512,1024,1536,2048,3072,4096,6144,8192,10240,12288,16384]
    for n in n_list:
        for b in b_list:
            for s in s_list:#[2,4,8,16,32,64,128,256,512,1024,2048,4096,8192,16384,32768,65536,131072]:
                for dtype in dtype_list:
                    for tp_size in [1,2,4,8,16,32,64]:
                        if b*s > 32768:
                            continue
                        # (input_len, batch_size, output_len, kv_cache_dtype, num_heads, world_size, tp_size, tokens_per_block, warming_up, test_ite, is_context_phase)
                        test_cases.append([s,b,1,dtype,n,tp_size,tp_size,64,10,6,True,"context_mla_perf.txt"])
    return test_cases

def get_generation_mla_test_cases():
    dtype_list = [tensorrt_llm.bindings.DataType.FP8,tensorrt_llm.bindings.DataType.BF16]
    test_cases = []
    n_list = [64, 128]
    for n in n_list:
        for b in [1,2,4,8,16,32,64,128,256,512,1024]:
            for s in [2,4,8,16,32,64,128,256,512,1024,2048,4096,8192,16384,32768,65536,131072]: # [target token s] is equivelant to [in: s-1, step=1]
                for dtype in dtype_list:
                    for tp_size in [1,2,4,8,16,32,64]:
                        if b*s > 1024*4096*2*2:
                            continue
                        # (input_len, batch_size, output_len, kv_cache_dtype, num_heads, world_size, tp_size, tokens_per_block, warming_up, test_ite, is_context_phase)
                        test_cases.append([s-1,b,1,dtype,n,tp_size,tp_size,64,10,6,False,"generation_mla_perf.txt"])
    return test_cases

def run_mla(input_len, batch_size, output_len, kv_cache_dtype, num_heads, world_size, tp_size, tokens_per_block, warming_up, test_ite, is_context_phase, perf_filename, device='cuda:0'):
    device = torch.device(device)
    torch.cuda.set_device(device)
    torch.set_default_device(device)
    backend_name = "TRTLLM"
    layer_idx = 0
    head_dim = 56
    num_key_value_heads = num_heads

    assert num_key_value_heads % tp_size == 0, "num_key_value_heads != N * tp_size"
    num_key_value_heads = int(num_key_value_heads/tp_size)

    assert num_heads % tp_size == 0, "num_heads != N * tp_size"
    num_heads = int(num_heads/tp_size)

    pos_embd_params = PositionalEmbeddingParams(
        type=PositionEmbeddingType.yarn,
        embedder=None, 
        rope=RopeParams(
            dim=56, 
            theta=10000, 
            scale_type=RotaryScalingType.yarn, 
            scale=40, 
            low_freq_factor=1.0, 
            high_freq_factor=4.0, 
            short_m_scale=1.0, 
            long_m_scale=1.0, 
            max_positions=163840, 
            original_max_positions=4096, 
            beta_fast=32, 
            beta_slow=1, 
            mscale=1.0, 
            mscale_all_dim=1.0))

    quant_config=QuantConfig(
        quant_algo='FP8_BLOCK_SCALES', 
        kv_cache_quant_algo=None, 
        group_size=None, 
        smoothquant_val=0.5, 
        clamp_val=None, 
        use_meta_recipe=False, 
        has_zero_point=False, 
        pre_quant_scale=False, 
        exclude_modules=None)
        
    q_lora_rank = 1536
    kv_lora_rank = 512
    qk_nope_head_dim = 128
    qk_rope_head_dim = 64
    v_head_dim = 128
    predicted_tokens_per_seq = 1

    attn = create_attention(backend_name = backend_name,
                            layer_idx = layer_idx,
                            num_heads = num_heads,
                            head_dim = (qk_nope_head_dim if is_context_phase else kv_lora_rank) + qk_rope_head_dim,
                            num_kv_heads = num_key_value_heads if is_context_phase else 1,
                            pos_embd_params=pos_embd_params,
                            quant_config=quant_config,
                            is_mla_enable=True,
                            q_lora_rank = q_lora_rank,
                            kv_lora_rank = kv_lora_rank,
                            qk_nope_head_dim = qk_nope_head_dim,
                            qk_rope_head_dim = qk_rope_head_dim,
                            v_head_dim = v_head_dim if is_context_phase else kv_lora_rank,
                            predicted_tokens_per_seq = predicted_tokens_per_seq)

    total_num_tokens = (input_len + output_len) * batch_size
    mapping = Mapping(world_size=world_size, rank=0, tp_size=tp_size)

    num_hidden_layers = 1

    kv_cache_config = KvCacheConfig(
        max_tokens=int((input_len + output_len - 1)/tokens_per_block + 1) * tokens_per_block * batch_size * 2, #num_bloacks * block_size
        enable_block_reuse=False)

    kv_cache_manager = KVCacheManager(
        kv_cache_config,
        tensorrt_llm.bindings.internal.batch_manager.CacheType.SELFKONLY,
        num_layers=num_hidden_layers,
        num_kv_heads=1,
        head_dim=kv_lora_rank + qk_rope_head_dim,
        tokens_per_block=tokens_per_block,
        max_seq_len=input_len + output_len,
        max_batch_size=batch_size,
        mapping=mapping,
        dtype=kv_cache_dtype)


    input_seq_lens = [input_len for _ in range(batch_size)]
    total_seq_lens = [input_len + output_len for _ in range(batch_size)]
    request_ids = [i for i in range(batch_size)]
    kv_cache_manager.add_dummy_requests(request_ids, total_seq_lens)

    if is_context_phase:
        num_cached_tokens_per_seq = [0 for _ in range(batch_size)]
        attn_metadata = TrtllmAttentionMetadata(max_num_requests=batch_size, 
                                max_num_tokens=total_num_tokens, 
                                kv_cache_manager=kv_cache_manager, 
                                mapping=mapping, 
                                enable_flash_mla=True,
                                seq_lens=torch.tensor(input_seq_lens, dtype=torch.int32), 
                                position_ids=None, 
                                num_contexts=batch_size,
                                kv_cache_params=KVCacheParams(use_cache=True, num_cached_tokens_per_seq=num_cached_tokens_per_seq, block_ids_per_seq=None, host_max_attention_window_sizes=None, host_sink_token_length=None), 
                                cross=None, 
                                request_ids=request_ids,
                                prompt_lens=input_seq_lens, 
                                runtime_features=AttentionRuntimeFeatures(chunked_prefill=False, cache_reuse=False), 
                                all_rank_num_tokens=None, 
                                workspace=torch.tensor([], device=device, dtype=torch.int8))
    else:
        gen_seq_lens = [1 for _ in range(batch_size)]
        attn_metadata = TrtllmAttentionMetadata(max_num_requests=batch_size, 
                                                max_num_tokens=total_num_tokens, 
                                                kv_cache_manager=kv_cache_manager, 
                                                mapping=mapping,
                                                enable_flash_mla=True,
                                                seq_lens=torch.tensor(gen_seq_lens, dtype=torch.int32), 
                                                position_ids=None, 
                                                num_contexts=0, 
                                                kv_cache_params=KVCacheParams(use_cache=True, num_cached_tokens_per_seq=input_seq_lens, block_ids_per_seq=None, host_max_attention_window_sizes=None, host_sink_token_length=None), 
                                                cross=None, 
                                                request_ids=request_ids, 
                                                prompt_lens=input_seq_lens, 
                                                runtime_features=AttentionRuntimeFeatures(chunked_prefill=False, cache_reuse=False), 
                                                all_rank_num_tokens=None, 
                                                workspace=torch.tensor([], device=device, dtype=torch.int8))

    attn_metadata.prepare()

    if is_context_phase:
        num_tokens = input_len * batch_size
    else:
        num_tokens = batch_size

    compressed_kv = torch.randn([num_tokens, kv_lora_rank], dtype=torch.bfloat16)
    k_pe = torch.randn([num_tokens, qk_rope_head_dim], dtype=torch.bfloat16)

    latent_cache = torch.cat([compressed_kv, k_pe], dim=-1)

    # self.qk_nope_head_dim + self.qk_rope_head_dim
    if is_context_phase:
        fused_q = torch.randn([
            num_tokens,
            num_heads * (qk_nope_head_dim + qk_rope_head_dim + qk_nope_head_dim + qk_rope_head_dim + v_head_dim) # 128 128 64
        ]).cuda().bfloat16()
        q_pe = None
    else:
        fused_q = torch.randn([
            num_tokens,
            num_heads * (kv_lora_rank + qk_rope_head_dim)
        ]).cuda().bfloat16()        
        q_pe = torch.randn([num_tokens, num_heads, qk_rope_head_dim]).cuda().bfloat16()

    # dry run
    if is_context_phase:
        attn.forward(
            fused_q,
            None,
            None,
            attn_metadata,
            out_scale=torch.tensor([], dtype=torch.float32),
            attention_input_type=AttentionInputType.context_only,
            latent_cache=latent_cache,
        )
    else:
        attn.forward(
            fused_q,
            None,
            None,
            attn_metadata,
            out_scale=torch.tensor([], dtype=torch.float32),
            attention_input_type=AttentionInputType.generation_only,
            latent_cache=latent_cache,
            q_pe=q_pe,
        )

    # capture
    g = torch.cuda.CUDAGraph()
    with torch.cuda.graph(g):
        if is_context_phase:
            attn.forward(
                fused_q,
                None,
                None,
                attn_metadata,
                out_scale=torch.tensor([], dtype=torch.float32),
                attention_input_type=AttentionInputType.context_only,
                latent_cache=latent_cache,
            )
        else:
            attn.forward(
                fused_q,
                None,
                None,
                attn_metadata,
                out_scale=torch.tensor([], dtype=torch.float32),
                attention_input_type=AttentionInputType.generation_only,
                latent_cache=latent_cache,
                q_pe=q_pe,
            )
    
    # warmup
    for i in range(warming_up):
        g.replay()

    # collect
    start_event = torch.cuda.Event(enable_timing=True)
    end_event = torch.cuda.Event(enable_timing=True)
    start_event.record()
    for i in range(test_ite):
        g.replay()
    end_event.record()
    torch.cuda.synchronize()
    latency = start_event.elapsed_time(end_event)/test_ite
    
    # write result
    if is_context_phase:
        isl = input_len
        step = 0
    else:
        isl = 1
        step = input_len

    dtype_str = 'float16'
    if kv_cache_dtype == tensorrt_llm.bindings.DataType.FP8:
        dtype_str = 'fp8'

    log_perf(item_list=[{ 
                        'mla_dtype': 'float16',
                        'kv_cache_dtype': dtype_str, 
                        'num_heads': num_heads,
                        'batch_size': batch_size,
                        'isl': isl,
                        'tp_size': tp_size,
                        'step': step, 
                        'latency': latency
                        }], 
            framework='TRTLLM', 
            version=tensorrt_llm.__version__, 
            device_name=torch.cuda.get_device_name(device), 
            op_name=f'mla_{"context" if is_context_phase else "generation"}', 
            kernel_source='default', 
            perf_filename=perf_filename)
